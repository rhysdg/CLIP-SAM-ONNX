{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8df0e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Available providers for ONNXRuntime: CPUExecutionProvider\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-384/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-384/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-384/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-384/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-384/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-384/resolve/main/processor_config.json HTTP/1.1\" 404 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "context: classification\n",
      "\n",
      "Probability that the image is 'a photo of a man': 0.212\n",
      "Probability that the image is 'a photo of a woman': 0.212\n",
      "Probability that the image is 's photo of a dog': 0.576\n",
      "\n",
      "context: situational\n",
      "\n",
      "Probability that the image is 'a dog standing up': 0.219\n",
      "Probability that the image is 'a dog running': 0.226\n",
      "Probability that the image is 'a dog laying on grass': 0.555\n"
     ]
    }
   ],
   "source": [
    "from clip.model import OnnxClip, softmax, get_similarity_scores\n",
    "from PIL import Image\n",
    "\n",
    "images = [Image.open(\"clip/data/dog.jpg\").convert(\"RGB\")]\n",
    "\n",
    "texts = {\"classification\": [\"a photo of a man\", \"a photo of a woman\", \"s photo of a dog\"],\n",
    "         \"situational\": [\"a dog standing up\", \"a dog running\", \"a dog laying on grass\"],\n",
    "     }\n",
    "\n",
    "\n",
    "onnx_model = OnnxClip(batch_size=16, type='siglip')\n",
    "\n",
    "image_embeddings = onnx_model.get_image_embeddings(images)\n",
    "text_embeddings_class = onnx_model.get_text_embeddings(texts['classification'])\n",
    "text_embeddings_situational = onnx_model.get_text_embeddings(texts['situational'])\n",
    "\n",
    "\n",
    "contexts = {\"classification\": text_embeddings_class,\n",
    "            \"situational\": text_embeddings_situational,\n",
    "           }\n",
    "\n",
    "logits = get_similarity_scores(image_embeddings, contexts)\n",
    "probabilities = softmax(logits['classification'])\n",
    "\n",
    "\n",
    "for k,v in contexts.items():\n",
    "    \n",
    "    print(f'\\ncontext: {k}\\n')\n",
    "    probabilities = softmax(logits[k])\n",
    "    \n",
    "    for text, p in zip(texts[k], probabilities[0]):\n",
    "        probabilities = softmax(logits['classification'])\n",
    "        print(f\"Probability that the image is '{text}': {p:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d39281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': array([[1.7257822e-05, 2.6411062e-06, 9.9998003e-01]], dtype=float32),\n",
       " 'situational': array([[0.01211032, 0.04465795, 0.9432317 ]], dtype=float32)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3447a691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c01bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import errno\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Iterable, Iterator, TypeVar, Optional\n",
    "import gdown\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "\n",
    "from clip import Preprocessor, Tokenizer\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes softmax values for each sets of scores in x.\n",
    "    This ensures the output sums to 1 for each image (along axis 1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Exponents\n",
    "    exp_arr = np.exp(x)\n",
    "\n",
    "    return exp_arr / np.sum(exp_arr, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def cosine_similarity(\n",
    "    embeddings_1: np.ndarray, embeddings_2: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the pairwise cosine similarities between two embedding arrays.\n",
    "\n",
    "    Args:\n",
    "        embeddings_1: An array of embeddings of shape (N, D).\n",
    "        embeddings_2: An array of embeddings of shape (M, D).\n",
    "\n",
    "    Returns:\n",
    "        An array of shape (N, M) with the pairwise cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    for embeddings in [embeddings_1, embeddings_2]:\n",
    "        if len(embeddings.shape) != 2:\n",
    "            raise ValueError(\n",
    "                f\"Expected 2-D arrays but got shape {embeddings.shape}.\"\n",
    "            )\n",
    "\n",
    "    d1 = embeddings_1.shape[1]\n",
    "    d2 = embeddings_2.shape[1]\n",
    "    if d1 != d2:\n",
    "        raise ValueError(\n",
    "            \"Expected second dimension of embeddings_1 and embeddings_2 to \"\n",
    "            f\"match, but got {d1} and {d2} respectively.\"\n",
    "        )\n",
    "\n",
    "    def normalize(embeddings):\n",
    "        return embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    embeddings_1 = normalize(embeddings_1)\n",
    "    embeddings_2 = normalize(embeddings_2)\n",
    "\n",
    "    return embeddings_1 @ embeddings_2.T\n",
    "\n",
    "\n",
    "def get_similarity_scores(image_embedding: list,\n",
    "                           queries: dict):\n",
    "    \"\"\"Compute pairwise similarity scores between two arrays of embeddings.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    res_dict = {}\n",
    "\n",
    "    for key, query in queries.items():\n",
    "      if not isinstance(query, (np.ndarray, np.generic) ):\n",
    "        continue\n",
    "\n",
    "      if image_embedding.ndim == 1:\n",
    "          # Convert to 2-D array using x[np.newaxis, :]\n",
    "          # and remove the extra dimension at the end.\n",
    "          res_dict[key] = softmax(get_similarity_scores(\n",
    "              image_embedding[np.newaxis, :], query\n",
    "          )[0])\n",
    "\n",
    "      if query.ndim == 1:\n",
    "          # Convert to 2-D array using x[np.newaxis, :]\n",
    "          # and remove the extra dimension at the end.\n",
    "          res_dict[key] = softmax(get_similarity_scores(\n",
    "              image_embedding, query[np.newaxis, :]\n",
    "          )[:, 0])\n",
    "\n",
    "      res_dict[key] = softmax(cosine_similarity(image_embedding, query) * 100)\n",
    "\n",
    "\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class OnnxClip2:\n",
    "    \"\"\"\n",
    "    This class can be utilised to predict the most relevant text snippet, given\n",
    "    an image, without directly optimizing for the task, similarly to the\n",
    "    zero-shot capabilities of GPT-2 and 3. The difference between this class\n",
    "    and [CLIP](https://github.com/openai/CLIP) is that here we don't depend on\n",
    "    `torch` or `torchvision`.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self, model: str = \"ViT-B/32\", batch_size: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiates the model and required encoding classes.\n",
    "\n",
    "        Args:\n",
    "            model: The model to utilise. Currently ViT-B/32 \n",
    "            batch_size: If set, splits the lists in `get_image_embeddings`\n",
    "                and `get_text_embeddings` into batches of this size before\n",
    "                passing them to the model. The embeddings are then concatenated\n",
    "                back together before being returned. This is necessary when\n",
    "                passing large amounts of data (perhaps ~100 or more).\n",
    "            \n",
    "        \"\"\" \n",
    "\n",
    "        providers = ort.get_available_providers()\n",
    "\n",
    "        if providers:\n",
    "            logging.info(\n",
    "                \"Available providers for ONNXRuntime: %s\", \", \".join(providers)\n",
    "            )\n",
    " \n",
    "\n",
    "        self.embedding_size = 512\n",
    "        self._model_urls = {'clip_image_model_vitb32.onnx': 'https://drive.google.com/file/d/1WbRBDaBLsVdAZRD_1deq0uYGhIVFNoAi/view?usp=drive_link',\n",
    "                            'clip_text_model_vitb32.onnx': 'https://drive.google.com/file/d/1EC2ju-gIlLfBJ3un-1G5QFQzYi8DoA9o/view?usp=drive_link'}\n",
    "\n",
    "        self.image_model, self.text_model = self._load_models(model)\n",
    "        self._tokenizer = Tokenizer()\n",
    "        self._preprocessor = Preprocessor()\n",
    "        self._batch_size = batch_size\n",
    "       \n",
    "    \n",
    "    @property\n",
    "    def EMBEDDING_SIZE(self):\n",
    "        raise RuntimeError(\"OnnxModel.EMBEDDING_SIZE is no longer supported,f please use the instance attribute: onnx_model.embedding_size\")\n",
    "\n",
    "\n",
    "    def _load_models(\n",
    "        self,\n",
    "        model: str,\n",
    "    ) -> Tuple[ort.InferenceSession, ort.InferenceSession]:\n",
    "      \n",
    "  \n",
    "        IMAGE_MODEL_FILE = \"clip_image_model_vitb32.onnx\"\n",
    "        TEXT_MODEL_FILE = \"clip_text_model_vitb32.onnx\"\n",
    "       \n",
    "        base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for model_file in [IMAGE_MODEL_FILE, TEXT_MODEL_FILE]:\n",
    "            path = os.path.join(base_dir, \"data\", model_file)\n",
    "            models.append(self._load_model(path))\n",
    "\n",
    "        return models[0], models[1]\n",
    "\n",
    "    def _load_model(self, path: str):\n",
    "        try:\n",
    "            if os.path.exists(path):\n",
    "                # `providers` need to be set explicitly since ORT 1.9\n",
    "                return ort.InferenceSession(\n",
    "                    path, providers=ort.get_available_providers()\n",
    "                )\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    errno.ENOENT,\n",
    "                    os.strerror(errno.ENOENT),\n",
    "                    path,\n",
    "                )\n",
    "        except FileNotFoundError:\n",
    "\n",
    "            gdown.download(url=self._model_urls[os.path.basename(path)], \n",
    "                            output=path, \n",
    "                            fuzzy=True\n",
    "                            )\n",
    "        \n",
    "            # `providers` need to be set explicitly since ORT 1.9\n",
    "            return ort.InferenceSession(\n",
    "                path, providers=ort.get_available_providers()\n",
    "            )\n",
    "\n",
    "    def get_image_embeddings(\n",
    "        self,\n",
    "        images: Iterable[Union[Image.Image, np.ndarray]],\n",
    "        with_batching: bool = True,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Compute the embeddings for a list of images.\n",
    "\n",
    "        Args:\n",
    "            images: A list of images to run on. Each image must be a 3-channel\n",
    "                (RGB) image. Can be any size, as the preprocessing step will\n",
    "                resize each image to size (224, 224).\n",
    "            with_batching: Whether to use batching - see the `batch_size` param\n",
    "                in `__init__()`\n",
    "\n",
    "        Returns:\n",
    "            An array of embeddings of shape (len(images), embedding_size).\n",
    "        \"\"\"\n",
    "        if not with_batching or self._batch_size is None:\n",
    "            # Preprocess images\n",
    "            images = [\n",
    "                self._preprocessor.encode_image(image) for image in images\n",
    "            ]\n",
    "            if not images:\n",
    "                return self._get_empty_embedding()\n",
    "\n",
    "            batch = np.concatenate(images)\n",
    "\n",
    "            return self.image_model.run(None, {\"IMAGE\": batch})[0]\n",
    "\n",
    "        else:\n",
    "            embeddings = []\n",
    "            for batch in to_batches(images, self._batch_size):\n",
    "                embeddings.append(\n",
    "                    self.get_image_embeddings(batch, with_batching=False)\n",
    "                )\n",
    "\n",
    "            if not embeddings:\n",
    "                return self._get_empty_embedding()\n",
    "\n",
    "            return np.concatenate(embeddings)\n",
    "\n",
    "    def get_text_embeddings(\n",
    "        self, texts: Iterable[str], with_batching: bool = True\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Compute the embeddings for a list of texts.\n",
    "\n",
    "        Args:\n",
    "            texts: A list of texts to run on. Each entry can be at most\n",
    "                77 characters.\n",
    "            with_batching: Whether to use batching - see the `batch_size` param\n",
    "                in `__init__()`\n",
    "\n",
    "        Returns:\n",
    "            An array of embeddings of shape (len(texts), embedding_size).\n",
    "        \"\"\"\n",
    "        if not with_batching or self._batch_size is None:\n",
    "            text = self._tokenizer.encode_text(texts)\n",
    "            if len(text) == 0:\n",
    "                return self._get_empty_embedding()\n",
    "\n",
    "            return self.text_model.run(None, {\"TEXT\": text})[0]\n",
    "        else:\n",
    "            embeddings = []\n",
    "            for batch in to_batches(texts, self._batch_size):\n",
    "                embeddings.append(\n",
    "                    self.get_text_embeddings(batch, with_batching=False)\n",
    "                )\n",
    "\n",
    "            if not embeddings:\n",
    "                return self._get_empty_embedding()\n",
    "\n",
    "            return np.concatenate(embeddings)\n",
    "\n",
    "    def _get_empty_embedding(self):\n",
    "        return np.empty((0, self.embedding_size), dtype=np.float32)\n",
    "    \n",
    "    def encode_text_with_prompt_ensemble(self, model, texts, device, prompt_templates=None):\n",
    "\n",
    "        # using default prompt templates for ImageNet\n",
    "        if prompt_templates == None:\n",
    "            prompt_templates = ['a bad photo of a {}.', 'a photo of many {}.', 'a sculpture of a {}.', 'a photo of the hard to see {}.', 'a low resolution photo of the {}.', 'a rendering of a {}.', 'graffiti of a {}.', 'a bad photo of the {}.', 'a cropped photo of the {}.', 'a tattoo of a {}.', 'the embroidered {}.', 'a photo of a hard to see {}.', 'a bright photo of a {}.', 'a photo of a clean {}.', 'a photo of a dirty {}.', 'a dark photo of the {}.', 'a drawing of a {}.', 'a photo of my {}.', 'the plastic {}.', 'a photo of the cool {}.', 'a close-up photo of a {}.', 'a black and white photo of the {}.', 'a painting of the {}.', 'a painting of a {}.', 'a pixelated photo of the {}.', 'a sculpture of the {}.', 'a bright photo of the {}.', 'a cropped photo of a {}.', 'a plastic {}.', 'a photo of the dirty {}.', 'a jpeg corrupted photo of a {}.', 'a blurry photo of the {}.', 'a photo of the {}.', 'a good photo of the {}.', 'a rendering of the {}.', 'a {} in a video game.', 'a photo of one {}.', 'a doodle of a {}.', 'a close-up photo of the {}.', 'a photo of a {}.', 'the origami {}.', 'the {} in a video game.', 'a sketch of a {}.', 'a doodle of the {}.', 'a origami {}.', 'a low resolution photo of a {}.', 'the toy {}.', 'a rendition of the {}.', 'a photo of the clean {}.', 'a photo of a large {}.', 'a rendition of a {}.', 'a photo of a nice {}.', 'a photo of a weird {}.', 'a blurry photo of a {}.', 'a cartoon {}.', 'art of a {}.', 'a sketch of the {}.', 'a embroidered {}.', 'a pixelated photo of a {}.', 'itap of the {}.', 'a jpeg corrupted photo of the {}.', 'a good photo of a {}.', 'a plushie {}.', 'a photo of the nice {}.', 'a photo of the small {}.', 'a photo of the weird {}.', 'the cartoon {}.', 'art of the {}.', 'a drawing of the {}.', 'a photo of the large {}.', 'a black and white photo of a {}.', 'the plushie {}.', 'a dark photo of a {}.', 'itap of a {}.', 'graffiti of the {}.', 'a toy {}.', 'itap of my {}.', 'a photo of a cool {}.', 'a photo of a small {}.', 'a tattoo of the {}.', 'there is a {} in the scene.', 'there is the {} in the scene.', 'this is a {} in the scene.', 'this is the {} in the scene.', 'this is one {} in the scene.']\n",
    "\n",
    "        text_features = []\n",
    "        for t in texts:\n",
    "            prompted_t = [template.format(t) for template in prompt_templates]\n",
    "            propmpted_t  = self._tokenizer.encode_text(prompted_t)\n",
    "            class_embeddings = self.text_model.run(None, {\"TEXT\": text})[0]\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            text_features.append(class_embedding)\n",
    "        text_features = torch.stack(text_features, dim=1).to(device).t()\n",
    "\n",
    "        return text_features\n",
    "\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "def to_batches(items: Iterable[T], size: int) -> Iterator[List[T]]:\n",
    "    \"\"\"\n",
    "    Splits an iterable (e.g. a list) into batches of length `size`. Includes\n",
    "    the last, potentially shorter batch.\n",
    "\n",
    "    Examples:\n",
    "        >>> list(to_batches([1, 2, 3, 4], size=2))\n",
    "        [[1, 2], [3, 4]]\n",
    "        >>> list(to_batches([1, 2, 3, 4, 5], size=2))\n",
    "        [[1, 2], [3, 4], [5]]\n",
    "\n",
    "        # To limit the number of batches returned\n",
    "        # (avoids reading the rest of `items`):\n",
    "        >>> import itertools\n",
    "        >>> list(itertools.islice(to_batches([1, 2, 3, 4, 5], size=2), 1))\n",
    "        [[1, 2]]\n",
    "\n",
    "    Args:\n",
    "        items: The iterable to split.\n",
    "        size: How many elements per batch.\n",
    "    \"\"\"\n",
    "    if size < 1:\n",
    "        raise ValueError(\"Chunk size must be positive.\")\n",
    "\n",
    "    batch = []\n",
    "    for item in items:\n",
    "        batch.append(item)\n",
    "\n",
    "        if len(batch) == size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "    # The last, potentially incomplete batch\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "\n",
    "\n",
    "def encode_text_with_prompt_ensemble(self, model, texts, device, prompt_templates=None):\n",
    "\n",
    "    # using default prompt templates for ImageNet\n",
    "    if prompt_templates == None:\n",
    "        prompt_templates = ['a bad photo of a {}.', 'a photo of many {}.', 'a sculpture of a {}.', 'a photo of the hard to see {}.', 'a low resolution photo of the {}.', 'a rendering of a {}.', 'graffiti of a {}.', 'a bad photo of the {}.', 'a cropped photo of the {}.', 'a tattoo of a {}.', 'the embroidered {}.', 'a photo of a hard to see {}.', 'a bright photo of a {}.', 'a photo of a clean {}.', 'a photo of a dirty {}.', 'a dark photo of the {}.', 'a drawing of a {}.', 'a photo of my {}.', 'the plastic {}.', 'a photo of the cool {}.', 'a close-up photo of a {}.', 'a black and white photo of the {}.', 'a painting of the {}.', 'a painting of a {}.', 'a pixelated photo of the {}.', 'a sculpture of the {}.', 'a bright photo of the {}.', 'a cropped photo of a {}.', 'a plastic {}.', 'a photo of the dirty {}.', 'a jpeg corrupted photo of a {}.', 'a blurry photo of the {}.', 'a photo of the {}.', 'a good photo of the {}.', 'a rendering of the {}.', 'a {} in a video game.', 'a photo of one {}.', 'a doodle of a {}.', 'a close-up photo of the {}.', 'a photo of a {}.', 'the origami {}.', 'the {} in a video game.', 'a sketch of a {}.', 'a doodle of the {}.', 'a origami {}.', 'a low resolution photo of a {}.', 'the toy {}.', 'a rendition of the {}.', 'a photo of the clean {}.', 'a photo of a large {}.', 'a rendition of a {}.', 'a photo of a nice {}.', 'a photo of a weird {}.', 'a blurry photo of a {}.', 'a cartoon {}.', 'art of a {}.', 'a sketch of the {}.', 'a embroidered {}.', 'a pixelated photo of a {}.', 'itap of the {}.', 'a jpeg corrupted photo of the {}.', 'a good photo of a {}.', 'a plushie {}.', 'a photo of the nice {}.', 'a photo of the small {}.', 'a photo of the weird {}.', 'the cartoon {}.', 'art of the {}.', 'a drawing of the {}.', 'a photo of the large {}.', 'a black and white photo of a {}.', 'the plushie {}.', 'a dark photo of a {}.', 'itap of a {}.', 'graffiti of the {}.', 'a toy {}.', 'itap of my {}.', 'a photo of a cool {}.', 'a photo of a small {}.', 'a tattoo of the {}.', 'there is a {} in the scene.', 'there is the {} in the scene.', 'this is a {} in the scene.', 'this is the {} in the scene.', 'this is one {} in the scene.']\n",
    "\n",
    "    text_features = []\n",
    "    for t in texts:\n",
    "        prompted_t = [template.format(t) for template in prompt_templates]\n",
    "        propmpted_t  = self._tokenizer.encode_text(prompted_t)\n",
    "        class_embeddings = self.text_model.run(None, {\"TEXT\": text})[0]\n",
    "        class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "        class_embedding = class_embeddings.mean(dim=0)\n",
    "        class_embedding /= class_embedding.norm()\n",
    "        text_features.append(class_embedding)\n",
    "    text_features = torch.stack(text_features, dim=1).to(device).t()\n",
    "\n",
    "    return text_features\n",
    "\n",
    "\n",
    "def get_similarity_map(sm, shape):\n",
    "\n",
    "    # min-max norm\n",
    "    sm = (sm - sm.min(1, keepdim=True)[0]) / (sm.max(1, keepdim=True)[0] - sm.min(1, keepdim=True)[0])\n",
    "\n",
    "    # reshape\n",
    "    side = int(sm.shape[1] ** 0.5) # square output\n",
    "    sm = sm.reshape(sm.shape[0], side, side, -1).permute(0, 3, 1, 2)\n",
    "\n",
    "    # interpolate\n",
    "    sm = torch.nn.functional.interpolate(sm, shape, mode='bilinear')\n",
    "    sm = sm.permute(0, 2, 3, 1)\n",
    "    \n",
    "    return sm\n",
    "\n",
    "\n",
    "def clip_feature_surgery(image_features, text_features, redundant_feats=None, t=2):\n",
    "\n",
    "    if redundant_feats != None:\n",
    "        similarity = image_features @ (text_features - redundant_feats).t()\n",
    "\n",
    "    else:\n",
    "        # weights to restrain influence of obvious classes on others\n",
    "        prob = image_features[:, :1, :] @ text_features.t()\n",
    "        prob = (prob * 2).softmax(-1)\n",
    "        w = prob / prob.mean(-1, keepdim=True)\n",
    "\n",
    "        # element-wise multiplied features\n",
    "        b, n_t, n_i, c = image_features.shape[0], text_features.shape[0], image_features.shape[1], image_features.shape[2]\n",
    "        feats = image_features.reshape(b, n_i, 1, c) * text_features.reshape(1, 1, n_t, c)\n",
    "        feats *= w.reshape(1, 1, n_t, 1)\n",
    "        redundant_feats = feats.mean(2, keepdim=True) # along cls dim\n",
    "        feats = feats - redundant_feats\n",
    "        \n",
    "        # sum the element-wise multiplied features as cosine similarity\n",
    "        similarity = feats.sum(-1)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# sm shape N_t\n",
    "def similarity_map_to_points(sm, shape, t=0.8, down_sample=2):\n",
    "    side = int(sm.shape[0] ** 0.5)\n",
    "    sm = sm.reshape(1, 1, side, side)\n",
    "\n",
    "    # down sample to smooth results\n",
    "    down_side = side // down_sample\n",
    "    sm = torch.nn.functional.interpolate(sm, (down_side, down_side), mode='bilinear')[0, 0, :, :]\n",
    "    h, w = sm.shape\n",
    "    sm = sm.reshape(-1)\n",
    "\n",
    "    sm = (sm - sm.min()) / (sm.max() - sm.min())\n",
    "    rank = sm.sort(0)[1]\n",
    "    scale_h = float(shape[0]) / h\n",
    "    scale_w = float(shape[1]) / w\n",
    "\n",
    "    num = min((sm >= t).sum(), sm.shape[0] // 2)\n",
    "    labels = np.ones(num * 2).astype('uint8')\n",
    "    labels[num:] = 0\n",
    "    points = []\n",
    "\n",
    "    # positives\n",
    "    for idx in rank[-num:]:\n",
    "        x = min((idx % w + 0.5) * scale_w, shape[1] - 1) # +0.5 to center\n",
    "        y = min((idx // w + 0.5) * scale_h, shape[0] - 1)\n",
    "        points.append([int(x.item()), int(y.item())])\n",
    "\n",
    "    # negatives\n",
    "    for idx in rank[:num]:\n",
    "        x = min((idx % w + 0.5) * scale_w, shape[1] - 1)\n",
    "        y = min((idx // w + 0.5) * scale_h, shape[0] - 1)\n",
    "        points.append([int(x.item()), int(y.item())])\n",
    "\n",
    "    return points, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e53ca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Available providers for ONNXRuntime: CPUExecutionProvider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 224, 224)\n",
      "(1, 512)\n",
      "Logits: {'ct1': array([[0.2674303 , 0.4776909 , 0.21659727, 0.03828153]], dtype=float32)}\n",
      "Probability that the image is 'bench': 0.251\n",
      "Probability that the image is 'person': 0.310\n",
      "Probability that the image is 'ground': 0.239\n",
      "Probability that the image is 'building': 0.200\n"
     ]
    }
   ],
   "source": [
    "from clip.model import OnnxClip, softmax, get_similarity_scores\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "all_texts = ['airplane', 'bag', 'bed', 'bedclothes', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'building', 'bus', 'cabinet', 'car', 'cat', 'ceiling', 'chair', 'cloth', 'computer', 'cow', 'cup', 'curtain', 'dog', 'door', 'fence', 'floor', 'flower', 'food', 'grass', 'ground', 'horse', 'keyboard', 'light', 'motorbike', 'mountain', 'mouse', 'person', 'plate', 'platform', 'potted plant', 'road', 'rock', 'sheep', 'shelves', 'sidewalk', 'sign', 'sky', 'snow', 'sofa', 'table', 'track', 'train', 'tree', 'truck', 'tv monitor', 'wall', 'water', 'window', 'wood']\n",
    "target_texts = ['bench', 'person', 'ground', 'building']\n",
    "\n",
    "# Your images/texts will get split into batches of this size before being\n",
    "# passed to CLIP, to limit memory usage\n",
    "onnx_model = OnnxClip(batch_size=16)\n",
    "\n",
    "# Unlike the original CLIP, there is no need to run tokenization/preprocessing\n",
    "# separately - simply run get_image_embeddings directly on PIL images/NumPy\n",
    "# arrays, and run get_text_embeddings directly on strings.\n",
    "image_embeddings = onnx_model.get_image_embeddings(images)\n",
    "\n",
    "encoded_image = onnx_model._preprocessor.encode_image(images[0])\n",
    "print(encoded_image.shape)\n",
    "print(image_embeddings.shape)\n",
    "text_embeddings = onnx_model.get_text_embeddings(target_texts)\n",
    "\n",
    "# To use the embeddings for zero-shot classification, you can use these two\n",
    "# functions. Here we run on a single image, but any number is supported.\n",
    "logits = get_similarity_scores(image_embeddings, {\"ct1\": text_embeddings})\n",
    "probabilities = softmax(logits['ct1'])\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "\n",
    "for text, p in zip(target_texts, probabilities[0]):\n",
    "    print(f\"Probability that the image is '{text}': {p:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f01129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "images = [Image.open(\"dog.jpg\").convert(\"RGB\")]\n",
    "image_embeddings = onnx_model.get_image_embeddings(images)\n",
    "image_embeddings = np.expand_dims(image_embeddings, axis=1)\n",
    "image_features = image_embeddings / LA.norm(image_embeddings, axis=1, keepdims=True)\n",
    "# Prompt ensemble for text features with normalization\n",
    "text_features = onnx_model.encode_text_with_prompt_ensemble(all_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde8e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = image_features @ text_features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58eabfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 59)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba32055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(1, 0, 59), dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b94e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = image_features @ text_features.t()\n",
    "similarity_map = clip.get_similarity_map(features[:, 1:, :], cv2_img.shape[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205179a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = onnx_model.get_image_embeddings(images)\n",
    "image_embeddings = np.expand_dims(image_embeddings, axis=1)\n",
    "image_features = image_embeddings / LA.norm(image_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c89621d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 512)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df8d620b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb789edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = onnx_model.get_image_embeddings(images)\n",
    "image_embeddings = np.expand_dims(image_embeddings, axis=1)\n",
    "image_features = image_embeddings / LA.norm(image_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb73a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_feature_surgery(image_features, text_features, redundant_feats=None, t=2):\n",
    "\n",
    "    if redundant_feats != None:\n",
    "        similarity = image_features @ (text_features - redundant_feats).t()\n",
    "\n",
    "    else:\n",
    "        # weights to restrain influence of obvious classes on others\n",
    "        prob = image_features[:, :1, :] @ text_features.t()\n",
    "        prob = (prob * 2).softmax(-1)\n",
    "        w = prob / prob.mean(-1, keepdim=True)\n",
    "\n",
    "        # element-wise multiplied features\n",
    "        b, n_t, n_i, c = image_features.shape[0], text_features.shape[0], image_features.shape[1], image_features.shape[2]\n",
    "        feats = image_features.reshape(b, n_i, 1, c) * text_features.reshape(1, 1, n_t, c)\n",
    "        feats *= w.reshape(1, 1, n_t, 1)\n",
    "        redundant_feats = feats.mean(2, keepdim=True) # along cls dim\n",
    "        feats = feats - redundant_feats\n",
    "        \n",
    "        # sum the element-wise multiplied features as cosine similarity\n",
    "        similarity = feats.sum(-1)\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d574236a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b37a743d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 't'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31000/3578652946.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_feature_surgery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31000/331019018.py\u001b[0m in \u001b[0;36mclip_feature_surgery\u001b[0;34m(image_features, text_features, redundant_feats, t)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# weights to restrain influence of obvious classes on others\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 't'"
     ]
    }
   ],
   "source": [
    "similarity = clip_feature_surgery(image_features, text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e4d945b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31000/2003185888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Extract image features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    # Extract image features\n",
    "    image_features = model.encode_image(image)\n",
    "    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # Prompt ensemble for text features with normalization\n",
    "    text_features = clip.encode_text_with_prompt_ensemble(model, all_texts, device)\n",
    "\n",
    "    # Similarity map from image tokens with min-max norm and resize, B,H,W,N\n",
    "    features = image_features @ text_features.t()\n",
    "    similarity_map = clip.get_similarity_map(features[:, 1:, :], cv2_img.shape[:2])\n",
    "\n",
    "    # Draw similarity map\n",
    "    for b in range(similarity_map.shape[0]):\n",
    "        for n in range(similarity_map.shape[-1]):\n",
    "            if all_texts[n] not in target_texts:\n",
    "                continue\n",
    "            vis = (similarity_map[b, :, :, n].cpu().numpy() * 255).astype('uint8')\n",
    "            vis = cv2.applyColorMap(vis, cv2.COLORMAP_JET)\n",
    "            vis = cv2_img * 0.4 + vis * 0.6\n",
    "            vis = cv2.cvtColor(vis.astype('uint8'), cv2.COLOR_BGR2RGB)\n",
    "            print('CLIP:', all_texts[n])\n",
    "            plt.imshow(vis)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8cffc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd84f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-224/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-224/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-224/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-224/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-224/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip-base-patch16-224/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "INFO:root:Available providers for ONNXRuntime: CPUExecutionProvider\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_61861/1570685856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# arrays, and run get_text_embeddings directly on strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mimage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/robots/clip-sam-onnx/clip/model.py\u001b[0m in \u001b[0;36mget_text_embeddings\u001b[0;34m(self, texts, with_batching)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 embeddings.append(\n\u001b[0;32m--> 319\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_batching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m                 )\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/robots/clip-sam-onnx/clip/model.py\u001b[0m in \u001b[0;36mget_text_embeddings\u001b[0;34m(self, texts, with_batching)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'siglip'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msiglip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_empty_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/robots/clip-sam-onnx/clip/tokenizer.py\u001b[0m in \u001b[0;36mencode_text\u001b[0;34m(self, texts, context_length, truncate, siglip)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msiglip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"np\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processor' is not defined"
     ]
    }
   ],
   "source": [
    "from clip.model import OnnxClip, softmax, get_similarity_scores\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "\n",
    "\n",
    "images = [Image.open(\"clip/data/dog.jpg\").convert(\"RGB\")]\n",
    "texts = [\"a photo of a man\", \"a photo of a woman\", \"s photo of a dog\"]\n",
    "\n",
    "# Your images/texts will get split into batches of this size before being\n",
    "# passed to CLIP, to limit memory usage\n",
    "onnx_model = OnnxClip(batch_size=16, type='siglip')\n",
    "\n",
    "# Unlike the original CLIP, there is no need to run tokenization/preprocessing\n",
    "# separately - simply run get_image_embeddings directly on PIL images/NumPy\n",
    "# arrays, and run get_text_embeddings directly on strings.\n",
    "image_embeddings = onnx_model.get_image_embeddings(images)\n",
    "text_embeddings = onnx_model.get_text_embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45c5a1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 576, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "011a4d57",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ct1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55840/1752942417.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ct1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logits:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ct1'"
     ]
    }
   ],
   "source": [
    "contexts = {'ct1': 'text_embeddings'}\n",
    "\n",
    "logits = get_similarity_scores(image_embeddings, contexts)\n",
    "probabilities = softmax(logits['ct1'])\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "\n",
    "for text, p in zip(texts, probabilities[0]):\n",
    "    print(f\"Probability that the image is '{text}': {p:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da1d3de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f98995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d7d3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 64, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88dd0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = cv2.imread('dog.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ae52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e883ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_similarity_map(sm, shape):\n",
    "\n",
    "    # min-max norm\n",
    "    print\n",
    "    sm = (sm - np.min(sm, axis=1, keepdims=True)[0]) / (np.max(sm, axis=1, keepdims=True)[0] - np.min(sm, axis=1, keepdims=True)[0])\n",
    "\n",
    "    # reshape\n",
    "    side = int(sm.shape[1] ** 0.5) # square output\n",
    "    print(side)\n",
    "    sm = sm.reshape(sm.shape[0], side, side, -1).transpose(0, 3, 1, 2)\n",
    "    print('shaaape')\n",
    "    print(sm.shape)\n",
    "    print(shape)\n",
    "\n",
    "\n",
    "    # interpolate\n",
    "    sm = torch.nn.functional.interpolate(torch.Tensor(sm), shape, mode='bilinear')\n",
    "    sm = sm.numpy()\n",
    "    sm = np.transpose(sm, (0, 2, 3, 1))\n",
    "    \n",
    "    return sm\n",
    "\n",
    "def clip_feature_surgery(image_features, text_features, redundant_feats=None, t=2):\n",
    "\n",
    "    if redundant_feats != None:\n",
    "        similarity = image_features @ (text_features - redundant_feats).T\n",
    "\n",
    "    else:\n",
    "        # weights to restrain influence of obvious classes on others\n",
    "        prob = image_features[:, :1, :] @ text_features.T\n",
    "        prob = (prob * 2).softmax(-1)\n",
    "        w = prob / prob.mean(-1, keepdim=True)\n",
    "\n",
    "        # element-wise multiplied features\n",
    "        b, n_t, n_i, c = image_features.shape[0], text_features.shape[0], image_features.shape[1], image_features.shape[2]\n",
    "        feats = image_features.reshape(b, n_i, 1, c) * text_features.reshape(1, 1, n_t, c)\n",
    "        feats *= w.reshape(1, 1, n_t, 1)\n",
    "        redundant_feats = feats.mean(2, keepdim=True) # along cls dim\n",
    "        feats = feats - redundant_feats\n",
    "        \n",
    "        # sum the element-wise multiplied features as cosine similarity\n",
    "        similarity = feats.sum(-1)\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6f44d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "images = [Image.open(\"demo.jpg\").convert(\"RGB\")]\n",
    "image_embeddings = onnx_model.get_image_embeddings(images)\n",
    "#image_embeddings = np.expand_dims(image_embeddings, axis=1)\n",
    "image_features = image_embeddings / LA.norm(image_embeddings, axis=1, keepdims=True)\n",
    "# Prompt ensemble for text features with normalization\n",
    "text_features = onnx_model.encode_text_with_prompt_ensemble(all_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90883c2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41190/3629015548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_embeddings\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeddings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msimilarity_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similarity_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "image_features = image_embeddings / LA.norm(image_embeddings, axis=1, keepdims=True)\n",
    "image_features = torch.tensor(image_embeddings) / torch.tensor(image_embeddings).norm(dim=1, keepdim=True)\n",
    "\n",
    "features = image_features @ text_features.T\n",
    "similarity_map = get_similarity_map(features[:, 1:, :].numpy(), np.array(images[0]).shape[:2])\n",
    "\n",
    "\n",
    "# Apply feature surgery\n",
    "similarity = clip_feature_surgery(image_features, text_features)\n",
    "#similarity_map = get_similarity_map(similarity[:, 1:, :], cv2_img.shape[:2])\n",
    "similarity_map = get_similarity_map(similarity[:, 1:, :].numpy(), cv2_img.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9791eecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib:matplotlib data path: /usr/share/matplotlib/mpl-data\n",
      "DEBUG:matplotlib:CONFIGDIR=/home/rhys/.config/matplotlib\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is linux\n",
      "DEBUG:matplotlib:loaded modules: ['sys', 'builtins', '_frozen_importlib', '_imp', '_thread', '_warnings', '_weakref', '_io', 'marshal', 'posix', '_frozen_importlib_external', 'time', 'zipimport', '_codecs', 'codecs', 'encodings.aliases', 'encodings', 'encodings.utf_8', '_signal', '_abc', 'abc', 'io', '__main__', '_stat', 'stat', '_collections_abc', 'genericpath', 'posixpath', 'os.path', 'os', '_sitebuiltins', '_distutils_hack', 'types', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib', 'importlib._abc', 'itertools', 'keyword', '_operator', 'operator', 'reprlib', '_collections', 'collections', '_functools', 'functools', 'contextlib', 'importlib.util', 'importlib.machinery', 'google', 'ruamel', 'ruamel.std', 'mpl_toolkits', 'apport_python_hook', 'sitecustomize', 'site', 'runpy', 'enum', '_sre', 'sre_constants', 'sre_parse', 'sre_compile', '_locale', 'copyreg', 're', 'ipykernel._version', '_json', 'json.scanner', 'json.decoder', 'json.encoder', 'json', 'errno', 'signal', '_weakrefset', 'threading', 'fcntl', '_posixsubprocess', 'select', 'collections.abc', 'math', 'selectors', 'subprocess', 'typing.io', 'typing.re', 'typing', 'jupyter_client._version', '_ast', 'ast', '_opcode', 'opcode', 'dis', 'token', 'tokenize', 'linecache', 'inspect', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets.utils.descriptions', 'traitlets.traitlets', 'weakref', 'copy', 'traitlets.utils.decorators', 'traitlets._version', 'traitlets', 'concurrent', 'traceback', '_string', 'string', 'atexit', 'logging', 'concurrent.futures._base', 'concurrent.futures', '_heapq', 'heapq', '_socket', 'array', 'socket', '_ssl', '_struct', 'struct', 'binascii', 'base64', 'ssl', 'asyncio.constants', 'asyncio.format_helpers', 'asyncio.base_futures', 'asyncio.log', 'asyncio.coroutines', '_contextvars', 'contextvars', 'asyncio.exceptions', 'asyncio.base_tasks', '_asyncio', 'asyncio.events', 'asyncio.futures', 'asyncio.protocols', 'asyncio.transports', 'asyncio.sslproto', 'asyncio.mixins', 'asyncio.tasks', 'asyncio.locks', 'asyncio.staggered', 'asyncio.trsock', 'asyncio.base_events', 'asyncio.runners', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.threads', 'asyncio.base_subprocess', 'asyncio.selector_events', 'asyncio.unix_events', 'asyncio', '_queue', 'queue', 'platform', '_ctypes', 'ctypes._endian', 'ctypes', 'zmq.backend.select', 'cython_runtime', 'zmq.backend.cython.constants', '_cython_0_29_28', 'zmq.backend.cython.error', 'zmq.error', 'zmq.backend.cython.message', 'zmq.backend.cython.context', '_bisect', 'bisect', '_random', '_sha512', 'random', '_compat_pickle', '_pickle', 'pickle', 'zmq.backend.cython.socket', 'zmq.backend.cython.utils', 'zmq.backend.cython._poll', 'zmq.backend.cython._version', 'zmq.backend.cython._device', 'zmq.backend.cython._proxy_steerable', 'zmq.backend.cython', 'zmq.backend', 'zmq.utils', 'zmq.utils.constant_names', 'zmq.sugar.constants', 'zmq.sugar.attrsettr', 'zmq.sugar.poll', 'zmq.utils.jsonapi', 'zmq.utils.strtypes', 'zmq.sugar.socket', 'zmq.sugar.context', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'zmq.sugar', 'zmq', 'zmq._future', 'zmq.asyncio', 'jupyter_client.channelsabc', '_hashlib', '_blake2', 'hashlib', 'hmac', 'dataclasses', 'pprint', '_datetime', 'datetime', 'gettext', 'argparse', 'traitlets.config.loader', 'textwrap', 'traitlets.utils.text', 'traitlets.config.configurable', 'traitlets.config.application', 'traitlets.config', 'traitlets.log', '__future__', 'tornado', 'logging.handlers', 'html.entities', 'html', 'urllib', 'urllib.parse', 'zlib', 'tornado.speedups', 'tornado.util', 'tornado.escape', 'colorama.ansi', 'colorama.win32', 'colorama.winterm', 'colorama.ansitowin32', 'colorama.initialise', 'colorama', '_curses', 'curses', 'tornado.log', 'numbers', 'tornado.concurrent', 'tornado.ioloop', 'tornado.platform', 'tornado.gen', 'tornado.platform.asyncio', 'zmq.eventloop.ioloop', 'zmq.eventloop', 'zmq.eventloop.zmqstream', 'jupyter_client.adapter', 'dateutil._version', 'dateutil', 'locale', 'calendar', 'six', '_decimal', 'decimal', 'dateutil._common', 'dateutil.relativedelta', 'six.moves', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.tz.tz', 'dateutil.tz', 'dateutil.parser._parser', 'dateutil.parser.isoparser', 'dateutil.parser', '_strptime', 'jupyter_client.jsonutil', 'jupyter_client.session', 'jupyter_client.channels', 'termios', 'getpass', 'jupyter_client.clientabc', 'fnmatch', 'glob', '_compression', '_bz2', 'bz2', '_lzma', 'lzma', 'shutil', 'tempfile', 'jupyter_core.version', 'jupyter_core', 'ntpath', 'pathlib', 'jupyter_core.paths', 'jupyter_client.localinterfaces', 'jupyter_client.utils', 'jupyter_client.connect', 'jupyter_client.client', 'jupyter_client.asynchronous.client', 'jupyter_client.asynchronous', 'jupyter_client.blocking.client', 'jupyter_client.blocking', 'jupyter_client.launcher', '_uuid', 'uuid', 'jupyter_client.managerabc', 'zipfile', 'configparser', 'entrypoints', 'jupyter_client.provisioning.provisioner_base', 'jupyter_client.provisioning.factory', 'jupyter_client.provisioning.local_provisioner', 'jupyter_client.provisioning', 'jupyter_client.kernelspec', 'jupyter_client.manager', 'jupyter_client.multikernelmanager', 'jupyter_client', 'ipykernel.connect', 'ipykernel', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'pkgutil', 'sysconfig', '_sysconfigdata__x86_64-linux-gnu', 'pydoc', 'bdb', 'IPython.utils', 'IPython.utils.ipstruct', 'IPython.utils.coloransi', 'pygments', 'IPython.utils.colorable', 'IPython.utils.PyColorize', 'IPython.utils.encoding', 'IPython.utils.py3compat', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'cmd', 'codeop', 'code', 'pdb', 'IPython.core.debugger', 'IPython.core.display_trap', 'pexpect.exceptions', 'pexpect.utils', 'pexpect.expect', 'tty', 'pty', 'resource', 'ptyprocess.util', 'ptyprocess.ptyprocess', 'ptyprocess', 'pexpect.spawnbase', 'pexpect.pty_spawn', 'pexpect.run', 'pexpect', 'shlex', 'IPython.utils._process_common', 'IPython.utils._process_posix', 'IPython.utils.process', 'IPython.utils.decorators', 'IPython.utils.path', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.core.ultratb', 'IPython.utils._sysinfo', 'IPython.utils.sysinfo', 'IPython.core.crashhandler', 'IPython.utils.importstring', 'IPython.paths', 'IPython.core.profiledir', 'IPython.core.application', 'IPython.terminal', 'IPython.core.compilerop', 'IPython.core.error', 'IPython.utils.text', 'IPython.core.magic_arguments', 'getopt', 'mimetypes', 'IPython.core.display', 'IPython.core.page', 'IPython.lib.security', 'IPython.lib', 'IPython.lib.pretty', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers._mapping', 'pygments.modeline', '_csv', 'csv', 'email', 'uu', 'quopri', 'email._parseaddr', 'email.base64mime', 'email.quoprimime', 'email.errors', 'email.encoders', 'email.charset', 'email.utils', 'email.header', 'email._policybase', 'email._encoded_words', 'email.iterators', 'email.message', 'importlib.metadata._functools', 'importlib.metadata._text', 'importlib.metadata._adapters', 'importlib.metadata._meta', 'importlib.metadata._collections', 'importlib.metadata._itertools', 'importlib.abc', 'importlib.metadata', 'pygments.plugin', 'pygments.util', 'pygments.lexers', 'pygments.filter', 'pygments.token', 'pygments.filters', 'pygments.regexopt', 'pygments.lexer', 'pygments.unistring', 'pygments.lexers.python', 'pygments.formatters._mapping', 'pygments.formatters', 'pygments.styles._mapping', 'pygments.styles', 'pygments.formatter', 'pygments.formatters.html', 'IPython.core.oinspect', 'IPython.core.inputtransformer2', 'decorator', 'IPython.core.magic', 'pickleshare', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.prefilter', 'IPython.core.alias', 'IPython.core.builtin_trap', 'backcall.backcall', 'backcall', 'IPython.core.events', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.utils.sentinel', 'IPython.core.formatters', '_sqlite3', 'sqlite3.dbapi2', 'sqlite3', 'IPython.core.history', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.lib.display', 'IPython.display', 'IPython.utils.capture', 'IPython.utils.io', 'IPython.core.hooks', 'IPython.utils.strdispatch', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.core.interactiveshell', 'prompt_toolkit.application.current', 'prompt_toolkit.eventloop.utils', 'prompt_toolkit.eventloop.async_generator', 'wcwidth.table_vs16', 'wcwidth.table_wide', 'wcwidth.table_zero', 'wcwidth.unicode_versions', 'wcwidth.wcwidth', 'wcwidth', 'prompt_toolkit.utils', 'prompt_toolkit.eventloop.inputhook', 'prompt_toolkit.eventloop', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.clipboard', 'prompt_toolkit.cache', 'prompt_toolkit.enums', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.app', 'prompt_toolkit.filters.cli', 'prompt_toolkit.filters.utils', 'prompt_toolkit.filters', 'prompt_toolkit.document', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.cursor_shapes', 'prompt_toolkit.data_structures', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.pygments', 'colorsys', 'prompt_toolkit.styles.style_transformation', 'prompt_toolkit.styles', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.base', 'prompt_toolkit.output.flush_stdout', 'prompt_toolkit.output.plain_text', 'prompt_toolkit.output.defaults', 'prompt_toolkit.output', 'prompt_toolkit.output.vt100', 'prompt_toolkit.mouse_events', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.formatted_text.ansi', 'xml', 'xml.dom.domreg', 'xml.dom', 'xml.dom.minicompat', 'xml.dom.NodeFilter', 'xml.dom.xmlbuilder', 'xml.dom.minidom', 'prompt_toolkit.formatted_text.html', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.formatted_text', 'prompt_toolkit.completion.base', 'prompt_toolkit.completion.deduplicate', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.completion.fuzzy_completer', 'prompt_toolkit.completion.nested', 'prompt_toolkit.completion', 'prompt_toolkit.history', 'prompt_toolkit.search', 'prompt_toolkit.validation', 'prompt_toolkit.buffer', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.lexers', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.controls', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.layout.screen', 'prompt_toolkit.layout.containers', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.layout.scrollable_pane', 'prompt_toolkit.layout', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.renderer', 'prompt_toolkit.application.application', 'prompt_toolkit.application.dummy', 'prompt_toolkit.application', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.widgets', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.shortcuts.utils', 'prompt_toolkit.shortcuts', 'prompt_toolkit', 'prompt_toolkit.patch_stdout', 'pygments.style', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'parso.utils', 'parso.tree', 'parso.python', 'parso.python.token', 'parso.python.tokenize', 'parso.pgen2.grammar_parser', 'parso.pgen2.generator', 'parso.pgen2', 'parso.parser', 'parso._compatibility', 'difflib', 'parso.python.prefix', 'parso.python.tree', 'parso.python.parser', 'parso.python.diff', 'gc', 'parso.cache', 'parso.normalizer', 'parso.python.errors', 'parso.python.pep8', 'parso.file_io', 'parso.grammar', 'parso', 'jedi._compatibility', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.file_io', 'jedi.inference.cache', 'jedi.inference.helpers', 'jedi.inference.utils', 'jedi.inference.base_value', 'jedi.inference.sys_path', 'jedi.inference.recursion', 'jedi.inference.flow_analysis', 'jedi.common', 'jedi.inference.lazy_value', 'jedi.inference.docstrings', 'jedi.plugins', 'jedi.inference.names', 'jedi.inference.filters', 'jedi.inference.compiled.getattr_static', 'jedi.inference.compiled.access', 'jedi.inference.signature', 'jedi.inference.context', 'jedi.inference.compiled.value', 'jedi.inference.compiled', 'jedi.inference.analysis', 'jedi.inference.gradual', 'jedi.inference.value.module', 'jedi.inference.value.dynamic_arrays', 'jedi.inference.value.iterable', 'jedi.inference.arguments', 'jedi.inference.parser_cache', 'jedi.inference.gradual.generics', 'jedi.inference.value.function', 'jedi.inference.value.klass', 'jedi.inference.value.instance', 'jedi.inference.value', 'jedi.inference.gradual.base', 'jedi.inference.gradual.type_var', 'jedi.inference.gradual.typing', 'jedi.inference.gradual.stub_value', 'jedi.inference.gradual.typeshed', 'jedi.inference.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.inference.compiled.subprocess', 'jedi.inference.imports', 'jedi.inference.param', 'jedi.inference.gradual.annotation', 'jedi.inference.value.decorator', 'jedi.inference.syntax_tree', 'jedi.inference', 'jedi.inference.gradual.conversion', 'jedi.inference.compiled.mixed', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.keywords', 'jedi.api.completion_cache', 'jedi.api.helpers', 'jedi.api.classes', 'jedi.api.interpreter', 'jedi.api.strings', 'jedi.api.file_name', 'jedi.api.completion', 'filecmp', 'jedi.api.environment', 'jedi.inference.references', 'jedi.api.project', 'jedi.api.errors', 'jedi.api.refactoring', 'jedi.api.refactoring.extract', 'jedi.inference.gradual.utils', 'jedi.api', 'jedi.plugins.stdlib', 'jedi.plugins.flask', 'jedi.plugins.pytest', 'jedi.plugins.django', 'jedi.plugins.registry', 'jedi', 'IPython.core.completer', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'concurrent.futures.thread', 'IPython.terminal.debugger', 'IPython.lib.clipboard', 'IPython.terminal.magics', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.interactiveshell', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'http', 'email.feedparser', 'email.parser', 'http.client', 'urllib.response', 'urllib.error', 'urllib.request', 'IPython.core.magics.code', 'IPython.core.magics.config', 'IPython.core.magics.display', 'timeit', '_lsprof', 'profile', 'cProfile', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.execution', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.packaging', 'IPython.core.pylabtools', 'IPython.core.magics.pylab', 'IPython.lib.backgroundjobs', 'IPython.core.magics.script', 'IPython.core.magics', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.terminal.ipapp', 'IPython.terminal.embed', 'IPython.utils.frame', 'IPython', 'imp', 'ipykernel.iostream', 'ipykernel.control', 'ipykernel.heartbeat', 'IPython.utils.tokenutil', 'pwd', 'psutil._common', 'psutil._compat', 'psutil._psposix', 'psutil._psutil_linux', 'psutil._psutil_posix', 'psutil._pslinux', 'psutil', 'tornado.locks', 'tornado.queues', 'ipykernel.jsonutil', 'ipykernel.kernelbase', 'ipykernel.comm.comm', 'ipykernel.comm.manager', 'ipykernel.comm', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.zmqshell', 'distutils.py39compat', 'distutils._functools', 'distutils.errors', 'distutils.sysconfig', 'distutils.command', 'distutils._log', 'distutils._collections', 'distutils._modified', 'distutils.dir_util', 'distutils.debug', 'distutils.spawn', 'grp', 'distutils.archive_util', 'distutils.file_util', 'distutils.util', 'distutils.cmd', 'distutils.config', 'distutils.fancy_getopt', 'distutils.dist', 'distutils.extension', 'distutils.core', 'distutils.command._framework_compat', 'distutils.command.install', 'distutils.command.install_egg_info', '_distutils_system_mod', 'setuptools._distutils', '_distutils_hack.override', 'distutils.log', 'distutils.filelist', 'setuptools.monkey', 'setuptools.logging', 'importlib._adapters', 'importlib._common', 'importlib.resources', 'setuptools._importlib', 'setuptools.version', 'setuptools._imp', 'setuptools.extern', 'setuptools._vendor', 'setuptools._vendor.packaging', 'setuptools.extern.packaging', 'setuptools.extern.packaging._structures', 'setuptools.extern.packaging.version', 'setuptools.depends', 'setuptools._path', 'setuptools.discovery', 'setuptools._vendor.more_itertools.recipes', 'setuptools._vendor.more_itertools.more', 'setuptools._vendor.more_itertools', 'setuptools.extern.more_itertools', 'setuptools._vendor.ordered_set', 'setuptools.extern.ordered_set', 'setuptools._vendor.packaging._elffile', 'setuptools._vendor.packaging._manylinux', 'setuptools._vendor.packaging._musllinux', 'setuptools.extern.packaging.tags', 'setuptools.extern.packaging.utils', 'setuptools.extern.packaging.specifiers', 'setuptools.extern.packaging._tokenizer', 'setuptools.extern.packaging._parser', 'setuptools.extern.packaging.markers', 'setuptools.errors', 'setuptools._vendor.jaraco', 'setuptools.extern.jaraco', 'setuptools.extern.jaraco.functools', 'setuptools._vendor.backports', 'setuptools.extern.backports', 'setuptools._vendor.backports.tarfile', 'setuptools.extern.jaraco.context', 'setuptools.extern.jaraco.text', 'setuptools._itertools', 'setuptools._entry_points', 'setuptools._normalization', 'setuptools.extern.packaging.requirements', 'setuptools._reqs', 'distutils.command.bdist', 'setuptools.command', 'setuptools.warnings', 'setuptools.config.expand', 'setuptools.config.setupcfg', 'setuptools.config', 'email._header_value_parser', 'email.headerregistry', 'setuptools.config._apply_pyprojecttoml', 'setuptools.config.pyprojecttoml', 'setuptools.dist', 'setuptools.extension', 'setuptools._core_metadata', 'setuptools.msvc', 'setuptools', 'distutils', 'distutils.version', 'ipykernel.eventloops', 'ipykernel.compiler', 'ipykernel.ipkernel', 'ipykernel.parentpoller', 'ipykernel.kernelapp', 'netifaces', 'faulthandler', 'IPython.core.completerlib', 'storemagic', 'numpy._utils', 'numpy._globals', 'numpy.exceptions', 'numpy._distributor_init', 'numpy.__config__', 'numpy._version', 'numpy.version', 'numpy._utils._inspect', 'numpy.core._exceptions', 'numpy.dtypes', 'numpy.core._multiarray_umath', 'numpy.core.overrides', 'numpy.core.multiarray', 'numpy.core.umath', 'numpy.core._string_helpers', 'numpy.compat.py3k', 'numpy.compat', 'numpy.core._dtype', 'numpy.core._type_aliases', 'numpy.core.numerictypes', 'numpy.core._ufunc_config', 'numpy.core._methods', 'numpy.core.fromnumeric', 'numpy.core.shape_base', 'numpy.core.arrayprint', 'numpy.core._asarray', 'numpy.core.numeric', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core._machar', 'numpy.core.getlimits', 'numpy.core.einsumfunc', 'numpy.core._multiarray_tests', 'numpy.core._add_newdocs', 'numpy.core._add_newdocs_scalars', 'numpy.core._dtype_ctypes', 'numpy.core._internal', 'numpy._pytesttester', 'numpy.core', 'numpy.lib.mixins', 'numpy.lib.ufunclike', 'numpy.lib.type_check', 'numpy.lib.scimath', 'numpy.lib.stride_tricks', 'numpy.lib.twodim_base', 'numpy.linalg._umath_linalg', 'numpy._typing._nested_sequence', 'numpy._typing._nbit', 'numpy._typing._char_codes', 'numpy._typing._scalars', 'numpy._typing._shape', 'numpy._typing._dtype_like', 'numpy._typing._array_like', 'numpy._typing', 'numpy.linalg.linalg', 'numpy.linalg', 'numpy.matrixlib.defmatrix', 'numpy.matrixlib', 'numpy.lib.histograms', 'numpy.lib.function_base', 'numpy.lib.index_tricks', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.polynomial', 'numpy.lib.utils', 'numpy.lib.arraysetops', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.npyio', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.lib', 'numpy.fft._pocketfft_internal', 'numpy.fft._pocketfft', 'numpy.fft.helper', 'numpy.fft', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.polynomial', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.polynomial', '_cython_0_29_36', 'numpy.random._common', 'secrets', 'numpy.random.bit_generator', 'numpy.random._bounded_integers', 'numpy.random._mt19937', 'numpy.random.mtrand', 'numpy.random._philox', 'numpy.random._pcg64', 'numpy.random._sfc64', 'numpy.random._generator', 'numpy.random._pickle', 'numpy.random', 'numpy.ctypeslib', 'numpy.ma.core', 'numpy.ma.extras', 'numpy.ma', 'numpy', 'cv2.load_config_py3', 'cv2.version', 'cv2.gapi.core', 'cv2.gapi.core.cpu', 'cv2.gapi.core.fluid', 'cv2.gapi.core.ocl', 'cv2.gapi.ie', 'cv2.gapi.ie.detail', 'cv2.gapi.imgproc', 'cv2.gapi.imgproc.fluid', 'cv2.gapi.oak', 'cv2.gapi.onnx', 'cv2.gapi.onnx.ep', 'cv2.gapi.ot', 'cv2.gapi.ot.cpu', 'cv2.gapi.ov', 'cv2.gapi.own', 'cv2.gapi.own.detail', 'cv2.gapi.render', 'cv2.gapi.render.ocv', 'cv2.gapi.streaming', 'cv2.gapi.video', 'cv2.gapi.wip', 'cv2.gapi.wip.draw', 'cv2.gapi.wip.gst', 'cv2.gapi.wip.onevpl', 'cv2.kinfu.detail', 'cv2.utils.fs', 'cv2.utils.nested', 'cv2.ximgproc.segmentation', 'cv2.ximgproc', 'cv2.parallel', 'cv2.videoio_registry', 'cv2.quality', 'cv2.line_descriptor', 'cv2.Error', 'cv2.xfeatures2d', 'cv2.phase_unwrapping', 'cv2.structured_light', 'cv2.datasets', 'cv2.cuda', 'cv2.img_hash', 'cv2.colored_kinfu', 'cv2.ipp', 'cv2.kinfu', 'cv2.wechat_qrcode', 'cv2.motempl', 'cv2.ft', 'cv2.optflow', 'cv2.intensity_transform', 'cv2.dpm', 'cv2.bioinspired', 'cv2.segmentation', 'cv2.reg', 'cv2.xphoto', 'cv2.ocl', 'cv2.plot', 'cv2.typing', 'cv2.ogl', 'cv2.videostab', 'cv2.fisheye', 'cv2.flann', 'cv2.data', 'cv2.rapid', 'cv2.face', 'cv2.rgbd', 'cv2.dnn_superres', 'cv2.large_kinfu', 'cv2.detail', 'cv2.linemod', 'cv2.multicalib', 'cv2.barcode', 'cv2.dynafu', 'cv2.signal', 'cv2.gapi', 'cv2.saliency', 'cv2.hfs', 'cv2.mat_wrapper', 'cv2.text', 'cv2.omnidir', 'cv2.dnn', 'cv2.misc.version', 'cv2.misc', 'cv2.ml', 'cv2.bgsegm', 'cv2.utils', 'cv2.stereo', 'cv2.qt', 'cv2.aruco', 'cv2.ccm', 'cv2.ppf_match_3d', 'cv2.legacy', 'cv2.mcc', 'cv2.samples', 'cv2', 'PIL._version', 'PIL', 'xml.parsers', 'pyexpat.errors', 'pyexpat.model', 'pyexpat', 'xml.parsers.expat.model', 'xml.parsers.expat.errors', 'xml.parsers.expat', 'defusedxml.common', 'defusedxml', 'xml.etree', 'xml.etree.ElementPath', '_elementtree', 'xml.etree.ElementTree', 'defusedxml.ElementTree', 'PIL.ExifTags', 'PIL.ImageMode', 'PIL.TiffTags', 'PIL._binary', 'PIL._util', 'PIL._imaging', 'cffi.lock', 'cffi.error', 'cffi.model', 'cffi.api', 'cffi', 'PIL.Image', 'clip.preprocessor', 'gzip', 'ftfy.bad_codecs', 'encodings.cp1250', 'encodings.cp1251', 'encodings.cp1252', 'encodings.cp1253', 'encodings.cp1254', 'encodings.cp1255', 'encodings.cp1256', 'encodings.cp1257', 'encodings.cp1258', 'encodings.iso8859_3', 'encodings.iso8859_6', 'encodings.iso8859_7', 'encodings.iso8859_8', 'encodings.iso8859_11', 'encodings.cp874', 'ftfy.bad_codecs.sloppy', 'encodings.iso8859_2', 'encodings.mac_roman', 'encodings.cp437', 'ftfy.chardata', 'ftfy.badness', 'ftfy.fixes', 'ftfy.formatting', 'ftfy', 'regex._regex', 'regex._regex_core', 'regex.regex', 'regex', 'clip.tokenizer', 'gdown.exceptions', 'filelock._error', 'filelock._api', 'filelock._util', 'filelock._soft', 'filelock._unix', 'filelock._windows', 'filelock.version', 'filelock', 'http.cookiejar', 'soupsieve.__meta__', 'soupsieve.util', 'soupsieve.pretty', 'soupsieve.css_types', 'soupsieve.css_match', 'soupsieve.css_parser', 'soupsieve', 'bs4.css', 'chardet.compat', 'chardet.enums', 'chardet.charsetprober', 'chardet.charsetgroupprober', 'chardet.codingstatemachine', 'chardet.escsm', 'chardet.escprober', 'chardet.latin1prober', 'chardet.mbcssm', 'chardet.utf8prober', 'chardet.mbcharsetprober', 'chardet.euctwfreq', 'chardet.euckrfreq', 'chardet.gb2312freq', 'chardet.big5freq', 'chardet.jisfreq', 'chardet.chardistribution', 'chardet.jpcntx', 'chardet.sjisprober', 'chardet.eucjpprober', 'chardet.gb2312prober', 'chardet.euckrprober', 'chardet.cp949prober', 'chardet.big5prober', 'chardet.euctwprober', 'chardet.mbcsgroupprober', 'chardet.sbcharsetprober', 'chardet.langcyrillicmodel', 'chardet.langgreekmodel', 'chardet.langbulgarianmodel', 'chardet.langthaimodel', 'chardet.langhebrewmodel', 'chardet.hebrewprober', 'chardet.langturkishmodel', 'chardet.sbcsgroupprober', 'chardet.universaldetector', 'chardet.version', 'chardet', 'bs4.dammit', 'bs4.formatter', 'bs4.element', '_markupbase', 'html.parser', 'bs4.builder._htmlparser', 'six.moves.urllib', 'webencodings.labels', 'encodings.utf_16_le', 'encodings.utf_16_be', 'webencodings', 'html5lib.constants', 'html5lib._utils', 'html5lib._inputstream', 'html5lib._trie._base', 'html5lib._trie.py', 'html5lib._trie', 'html5lib._tokenizer', 'html5lib.treebuilders', 'html5lib.treebuilders.base', 'html5lib.html5parser', 'html5lib.treewalkers', 'xml.sax.handler', 'xml.sax._exceptions', 'xml.sax.xmlreader', 'xml.sax', 'xml.sax.saxutils', 'html5lib.serializer', 'html5lib', 'bs4.builder._html5lib', 'lxml', 'lxml._elementpath', 'lxml.etree', 'bs4.builder._lxml', 'bs4.builder', 'bs4', 'urllib3.exceptions', 'urllib3.util.timeout', 'urllib3.util.connection', 'urllib3.util.util', '_brotli', 'brotli', 'urllib3.util.request', 'urllib3.util.response', 'urllib3.util.retry', 'urllib3.util.url', 'urllib3.util.ssltransport', 'urllib3.util.ssl_', 'urllib3.util.wait', 'urllib3.util', 'urllib3._base_connection', 'urllib3._collections', 'urllib3._version', 'urllib3.fields', 'urllib3.filepost', 'ipaddress', 'urllib3.util.ssl_match_hostname', 'urllib3.connection', 'urllib3.response', 'urllib3._request_methods', 'urllib3.util.proxy', 'urllib3.connectionpool', 'urllib3.poolmanager', 'urllib3', 'http.cookies', 'requests.compat', 'requests.exceptions', 'charset_normalizer.assets', 'charset_normalizer.constant', 'charset_normalizer.md__mypyc', '_multibytecodec', 'charset_normalizer.utils', 'charset_normalizer.md', 'charset_normalizer.models', 'charset_normalizer.cd', 'charset_normalizer.api', 'charset_normalizer.legacy', 'charset_normalizer.version', 'charset_normalizer', 'requests.packages.urllib3.exceptions', 'requests.packages.urllib3.util.timeout', 'requests.packages.urllib3.util.connection', 'requests.packages.urllib3.util.util', 'requests.packages.urllib3.util.request', 'requests.packages.urllib3.util.response', 'requests.packages.urllib3.util.retry', 'requests.packages.urllib3.util.url', 'requests.packages.urllib3.util.ssltransport', 'requests.packages.urllib3.util.ssl_', 'requests.packages.urllib3.util.wait', 'requests.packages.urllib3.util', 'requests.packages.urllib3._base_connection', 'requests.packages.urllib3._collections', 'requests.packages.urllib3._version', 'requests.packages.urllib3.fields', 'requests.packages.urllib3.filepost', 'requests.packages.urllib3.util.ssl_match_hostname', 'requests.packages.urllib3.connection', 'requests.packages.urllib3.response', 'requests.packages.urllib3._request_methods', 'requests.packages.urllib3.util.proxy', 'requests.packages.urllib3.connectionpool', 'requests.packages.urllib3.poolmanager', 'requests.packages.urllib3', 'idna.package_data', 'idna.idnadata', 'idna.intranges', 'idna.core', 'idna', 'requests.packages.idna.package_data', 'requests.packages.idna.idnadata', 'requests.packages.idna.intranges', 'requests.packages.idna.core', 'requests.packages.idna', 'requests.packages.chardet', 'requests.packages', 'certifi.core', 'certifi', 'requests.certs', 'requests.__version__', 'requests._internal_utils', 'requests.cookies', 'requests.structures', 'importlib.readers', 'requests.utils', 'requests.auth', 'stringprep', 'encodings.idna', 'requests.hooks', 'requests.status_codes', 'requests.models', 'urllib3.contrib', 'socks', 'urllib3.contrib.socks', 'requests.adapters', 'requests.sessions', 'requests.api', 'requests', 'tqdm._monitor', 'tqdm._tqdm_pandas', 'tqdm.utils', 'tqdm.std', 'tqdm._dist_ver', 'tqdm.version', 'tqdm.cli', 'tqdm.gui', 'tqdm', 'gdown._indent', 'gdown.parse_url', 'gdown.download', 'gdown.cached_download', 'gdown.download_folder', 'tarfile', 'gdown.extractall', 'gdown', 'onnxruntime.capi', 'onnxruntime.capi._ld_preload', 'onnxruntime.capi.onnxruntime_pybind11_state', 'onnxruntime.capi._pybind_state', 'onnxruntime.capi.onnxruntime_validation', 'onnxruntime.capi.onnxruntime_inference_collection', 'onnxruntime.capi.training', 'onnxruntime', 'clip.model', 'clip', 'PIL.ImageFile', 'PIL.GimpGradientFile', 'PIL.GimpPaletteFile', 'PIL.ImageColor', 'PIL.PaletteFile', 'PIL.ImagePalette', 'PIL.BmpImagePlugin', 'PIL.ImageChops', 'PIL.ImageSequence', 'PIL.GifImagePlugin', 'PIL.JpegPresets', 'PIL.JpegImagePlugin', 'PIL.PpmImagePlugin', 'PIL.PngImagePlugin', 'matplotlib', 'packaging', 'packaging._structures', 'packaging.version', 'matplotlib._api.deprecation', 'matplotlib._api', 'matplotlib._version', 'matplotlib._c_internal_utils', 'matplotlib.cbook', 'matplotlib.docstring', 'matplotlib._path', 'matplotlib.bezier', 'matplotlib.path', 'matplotlib.transforms', 'matplotlib.ticker', 'matplotlib.scale', 'matplotlib._color_data', 'matplotlib.colors', 'pyparsing', 'matplotlib.fontconfig_pattern', 'matplotlib._enums', 'cycler', 'matplotlib.rcsetup', 'matplotlib.ft2font', 'kiwisolver']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib:CACHEDIR=/home/rhys/.cache/matplotlib\n",
      "DEBUG:matplotlib.font_manager:Using fontManager instance from /home/rhys/.cache/matplotlib/fontlist-v330.json\n",
      "DEBUG:matplotlib.pyplot:Loaded backend module://matplotlib_inline.backend_inline version unknown.\n",
      "DEBUG:matplotlib.pyplot:Loaded backend module://matplotlib_inline.backend_inline version unknown.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41190/1162943835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcv2_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Draw similarity map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv2_img = np.array(images[0])\n",
    "# Draw similarity map\n",
    "for b in range(similarity_map.shape[0]):\n",
    "    for n in range(similarity_map.shape[-1]):\n",
    "        if all_texts[n] not in target_texts:\n",
    "            continue\n",
    "        vis = (similarity_map[b, :, :, n] * 255).astype('uint8')\n",
    "        vis = cv2.applyColorMap(vis, cv2.COLORMAP_JET)\n",
    "        vis = cv2_img * 0.4 + vis * 0.6\n",
    "        vis = cv2.cvtColor(vis.astype('uint8'), cv2.COLOR_BGR2RGB)\n",
    "        print('CLIP:', all_texts[n])\n",
    "        plt.imshow(vis)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bac5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_map(sm, shape):\n",
    "\n",
    "    # min-max norm\n",
    "    sm = (sm - sm.min(1, keepdim=True)[0]) / (sm.max(1, keepdim=True)[0] - sm.min(1, keepdim=True)[0])\n",
    "\n",
    "    # reshape\n",
    "    side = int(sm.shape[1] ** 0.5) # square output\n",
    "    sm = sm.reshape(sm.shape[0], side, side, -1).permute(0, 3, 1, 2)\n",
    "\n",
    "    # interpolate\n",
    "    sm = torch.nn.functional.interpolate(sm, shape, mode='bilinear')\n",
    "    sm = sm.permute(0, 2, 3, 1)\n",
    "    \n",
    "    return sm\n",
    "\n",
    "\n",
    "def clip_feature_surgery(image_features, text_features, redundant_feats=None, t=2):\n",
    "\n",
    "    if redundant_feats != None:\n",
    "        similarity = image_features @ (text_features - redundant_feats).t()\n",
    "\n",
    "    else:\n",
    "        # weights to restrain influence of obvious classes on others\n",
    "        prob = image_features[:, :1, :] @ text_features.t()\n",
    "        prob = (prob * 2).softmax(-1)\n",
    "        w = prob / prob.mean(-1, keepdim=True)\n",
    "\n",
    "        # element-wise multiplied features\n",
    "        b, n_t, n_i, c = image_features.shape[0], text_features.shape[0], image_features.shape[1], image_features.shape[2]\n",
    "        feats = image_features.reshape(b, n_i, 1, c) * text_features.reshape(1, 1, n_t, c)\n",
    "        feats *= w.reshape(1, 1, n_t, 1)\n",
    "        redundant_feats = feats.mean(2, keepdim=True) # along cls dim\n",
    "        feats = feats - redundant_feats\n",
    "        \n",
    "        # sum the element-wise multiplied features as cosine similarity\n",
    "        similarity = feats.sum(-1)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# sm shape N_t\n",
    "def similarity_map_to_points(sm, shape, t=0.8, down_sample=2):\n",
    "    side = int(sm.shape[0] ** 0.5)\n",
    "    sm = sm.reshape(1, 1, side, side)\n",
    "\n",
    "    # down sample to smooth results\n",
    "    down_side = side // down_sample\n",
    "    sm = torch.nn.functional.interpolate(sm, (down_side, down_side), mode='bilinear')[0, 0, :, :]\n",
    "    h, w = sm.shape\n",
    "    sm = sm.reshape(-1)\n",
    "\n",
    "    sm = (sm - sm.min()) / (sm.max() - sm.min())\n",
    "    rank = sm.sort(0)[1]\n",
    "    scale_h = float(shape[0]) / h\n",
    "    scale_w = float(shape[1]) / w\n",
    "\n",
    "    num = min((sm >= t).sum(), sm.shape[0] // 2)\n",
    "    labels = np.ones(num * 2).astype('uint8')\n",
    "    labels[num:] = 0\n",
    "    points = []\n",
    "\n",
    "    # positives\n",
    "    for idx in rank[-num:]:\n",
    "        x = min((idx % w + 0.5) * scale_w, shape[1] - 1) # +0.5 to center\n",
    "        y = min((idx // w + 0.5) * scale_h, shape[0] - 1)\n",
    "        points.append([int(x.item()), int(y.item())])\n",
    "\n",
    "    # negatives\n",
    "    for idx in rank[:num]:\n",
    "        x = min((idx % w + 0.5) * scale_w, shape[1] - 1)\n",
    "        y = min((idx // w + 0.5) * scale_h, shape[0] - 1)\n",
    "        points.append([int(x.item()), int(y.item())])\n",
    "\n",
    "    return points, labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
